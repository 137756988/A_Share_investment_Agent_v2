"""
è´¢åŠ¡æŠ¥å‘Šåˆ†æåŠ©æ‰‹æ¨¡å—

æ­¤æ¨¡å—è´Ÿè´£è§£æç”Ÿæˆçš„æŠ•èµ„åˆ†ææŠ¥å‘Šï¼Œå°†å…¶ç¿»è¯‘æˆä¸­æ–‡å¹¶æä¾›æ˜“äºç†è§£çš„è§£è¯»
"""

import os
import logging
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple

# å¯¼å…¥å·¥å…·å’Œå®¢æˆ·ç«¯
from src.agents.state import AgentState
from src.tools.openrouter_config import get_chat_completion
from src.utils.api_utils import agent_endpoint

# è®¾ç½®æ—¥å¿—è®°å½•
logger = logging.getLogger("report_analyzer_agent")

class ReportSection:
    """æŠ¥å‘Šç« èŠ‚ç±»ï¼Œç”¨äºå­˜å‚¨å’Œå¤„ç†æŠ¥å‘Šä¸­çš„å„ä¸ªéƒ¨åˆ†"""
    
    def __init__(self, name: str, content: str = "", confidence: Optional[float] = None):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç« èŠ‚
        
        Args:
            name: ç« èŠ‚åç§°
            content: ç« èŠ‚å†…å®¹
            confidence: ç« èŠ‚ç›¸å…³çš„ç½®ä¿¡åº¦ï¼ˆå¦‚æœ‰ï¼‰
        """
        self.name = name
        self.content = content
        self.confidence = confidence
    
    def __str__(self) -> str:
        """è¿”å›ç« èŠ‚çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        if self.confidence is not None:
            return f"{self.name} (ç½®ä¿¡åº¦: {self.confidence:.0%}): {self.content}"
        return f"{self.name}: {self.content}"

def extract_sections(log_content: str) -> List[ReportSection]:
    """
    ä»ç»“æ„åŒ–æ—¥å¿—ä¸­æå–å„ä¸ªç« èŠ‚
    
    Args:
        log_content: æ—¥å¿—å†…å®¹æ–‡æœ¬
        
    Returns:
        æå–çš„æŠ¥å‘Šç« èŠ‚åˆ—è¡¨
    """
    sections = []
    
    # å®šä¹‰å¯èƒ½çš„ç« èŠ‚åç§°å’Œæ¨¡å¼ï¼ˆä¿®æ”¹ä¸ºåŒ¹é…ä¸­æ–‡æ¨¡å¼ï¼‰
    section_patterns = [
        (r"æŠ€æœ¯åˆ†æ.*ä¿¡å·:.*(\w+).*ç½®ä¿¡åº¦:.*(\d+)%", "æŠ€æœ¯åˆ†æ"),
        (r"åŸºæœ¬é¢åˆ†æ.*ä¿¡å·:.*(\w+).*ç½®ä¿¡åº¦:.*(\d+)%", "åŸºæœ¬é¢åˆ†æ"),
        (r"æƒ…æ„Ÿåˆ†æ.*ä¿¡å·:.*(\w+).*ç½®ä¿¡åº¦:.*(\d+)%", "æƒ…æ„Ÿåˆ†æ"),
        (r"ä¼°å€¼åˆ†æ.*ä¿¡å·:.*(\w+).*ç½®ä¿¡åº¦:.*(\d+)%", "ä¼°å€¼åˆ†æ"),
        (r"å¤šæ–¹ç ”ç©¶.*ç½®ä¿¡åº¦:.*(\d+)%", "å¤šæ–¹ç ”ç©¶"),
        (r"ç©ºæ–¹ç ”ç©¶.*ç½®ä¿¡åº¦:.*(\d+)%", "ç©ºæ–¹ç ”ç©¶"),
        (r"è¾©è®ºå®¤åˆ†æ.*ä¿¡å·:.*(\w+).*ç½®ä¿¡åº¦:.*(\d+)%", "è¾©è®ºå®¤åˆ†æ"),
        (r"é£é™©ç®¡ç†åˆ†æ.*æœ€å¤§ä»“ä½:.*[\d\.]+.*é£é™©è¯„åˆ†:.*\d+", "é£é™©ç®¡ç†åˆ†æ"),
        (r"å®è§‚åˆ†æ.*å®è§‚ç¯å¢ƒ:.*(\w+).*å½±å“:.*(\w+)", "å®è§‚åˆ†æ"),
        (r"æŠ•èµ„ç»„åˆç®¡ç†åˆ†æ.*äº¤æ˜“è¡ŒåŠ¨:.*(\w+).*å†³ç­–ä¿¡å¿ƒ:.*(\d+)%", "æŠ•èµ„ç»„åˆç®¡ç†åˆ†æ")
    ]
    
    # æå–å„ä¸ªç« èŠ‚
    current_section = None
    current_content = []
    
    lines = log_content.split('\n')
    for line in lines:
        matched = False
        for pattern, section_name in section_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                # å¦‚æœå·²æœ‰ç« èŠ‚ï¼Œä¿å­˜å®ƒ
                if current_section:
                    sections.append(ReportSection(
                        current_section, 
                        '\n'.join(current_content).strip()
                    ))
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_section = section_name
                current_content = [line]
                matched = True
                break
        
        if not matched and current_section:
            current_content.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
    if current_section:
        sections.append(ReportSection(
            current_section, 
            '\n'.join(current_content).strip()
        ))
    
    # å¦‚æœæœªæå–åˆ°ä»»ä½•ç« èŠ‚ï¼Œå°è¯•æå–åŸºäºåˆ†éš”ç¬¦çš„ç« èŠ‚
    if not sections:
        logger.warning("æœªä½¿ç”¨æ­£åˆ™æ¨¡å¼æå–åˆ°ä»»ä½•ç« èŠ‚ï¼Œå°è¯•ä½¿ç”¨åˆ†éš”ç¬¦æå–...")
        
        section_markers = [
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ“ˆ æŠ€æœ¯åˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ“ åŸºæœ¬é¢åˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ” æƒ…æ„Ÿåˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ’° ä¼°å€¼åˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ‚ å¤šæ–¹ç ”ç©¶åˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ» ç©ºæ–¹ç ”ç©¶åˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ—£ï¸ è¾©è®ºå®¤åˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• âš ï¸ é£é™©ç®¡ç†åˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸŒ å®è§‚åˆ†æåˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ğŸ“‚ æŠ•èµ„ç»„åˆç®¡ç†åˆ†æ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        ]
        
        section_names = [
            "æŠ€æœ¯åˆ†æ", "åŸºæœ¬é¢åˆ†æ", "æƒ…æ„Ÿåˆ†æ", "ä¼°å€¼åˆ†æ", 
            "å¤šæ–¹ç ”ç©¶", "ç©ºæ–¹ç ”ç©¶", "è¾©è®ºå®¤åˆ†æ", 
            "é£é™©ç®¡ç†åˆ†æ", "å®è§‚åˆ†æ", "æŠ•èµ„ç»„åˆç®¡ç†åˆ†æ"
        ]
        
        # ä¸ºæ¯ä¸ªç« èŠ‚æŸ¥æ‰¾å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®
        for i, marker in enumerate(section_markers):
            try:
                start_idx = log_content.index(marker)
                end_marker = "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                end_idx = log_content.find(end_marker, start_idx)
                
                if end_idx > start_idx:
                    section_content = log_content[start_idx:end_idx + len(end_marker)]
                    sections.append(ReportSection(section_names[i], section_content))
                    logger.info(f"æ‰¾åˆ°ç« èŠ‚: {section_names[i]}")
            except ValueError:
                continue
    
    return sections

def parse_confidence(section_content: str) -> Optional[float]:
    """
    ä»ç« èŠ‚å†…å®¹ä¸­è§£æç½®ä¿¡åº¦
    
    Args:
        section_content: ç« èŠ‚å†…å®¹æ–‡æœ¬
        
    Returns:
        è§£æå‡ºçš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´0-1ï¼Œå¦‚æœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    confidence_pattern = r"ç½®ä¿¡åº¦:.*?(\d+)%"
    match = re.search(confidence_pattern, section_content)
    if match:
        confidence_str = match.group(1)
        try:
            return int(confidence_str) / 100.0
        except ValueError:
            return None
    return None

def extract_final_decision(log_content: str) -> Tuple[str, Optional[float]]:
    """
    ä»æ—¥å¿—ä¸­æå–æœ€ç»ˆæŠ•èµ„å†³ç­–
    
    Args:
        log_content: æ—¥å¿—å†…å®¹æ–‡æœ¬
        
    Returns:
        å…ƒç»„: (å†³ç­–è¡ŒåŠ¨, ç½®ä¿¡åº¦)
    """
    # å°è¯•å¤šç§æ¨¡å¼æ¥åŒ¹é…å†³ç­–
    decision_patterns = [
        r"äº¤æ˜“è¡ŒåŠ¨:.*?(\w+).*?å†³ç­–ä¿¡å¿ƒ:.*?(\d+)%",
        r"action\"\s*:\s*\"(\w+)\".*?confidence\"\s*:\s*(\d+)",
        r"action\"\s*:\s*\"(\w+)",
    ]
    
    for pattern in decision_patterns:
        match = re.search(pattern, log_content, re.IGNORECASE)
        if match:
            action = match.group(1)
            try:
                confidence = int(match.group(2)) / 100.0
            except (IndexError, ValueError):
                confidence = None
            return action, confidence
    
    # å¦‚æœæœªæ‰¾åˆ°å†³ç­–ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é£é™©ç®¡ç†éƒ¨åˆ†çš„äº¤æ˜“è¡ŒåŠ¨
    risk_pattern = r"trading_action\"\s*:\s*\"(\w+)"
    match = re.search(risk_pattern, log_content)
    if match:
        return match.group(1), None
    
    return "æœªçŸ¥", None

def ensure_correct_report_title(report_content: str, ticker: str, stock_name: str) -> str:
    """
    ç¡®ä¿æŠ¥å‘Šæ ‡é¢˜åŒ…å«æ­£ç¡®çš„è‚¡ç¥¨ä»£ç å’Œåç§°
    
    Args:
        report_content: æŠ¥å‘Šå†…å®¹
        ticker: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        
    Returns:
        ä¿®æ­£åçš„æŠ¥å‘Šå†…å®¹
    """
    # ç¡®ä¿è‚¡ç¥¨ä»£ç å’Œåç§°æ˜¯å­—ç¬¦ä¸²
    ticker = str(ticker)
    stock_name = str(stock_name)
    
    # æ£€æŸ¥æŠ¥å‘Šçš„ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«æ­£ç¡®çš„æ ‡é¢˜
    lines = report_content.split('\n')
    if not lines:
        return report_content
    
    # é¢„æœŸçš„æ ‡é¢˜æ ¼å¼
    expected_title = f"# {ticker} {stock_name}æŠ•èµ„åˆ†ææŠ¥å‘Š"
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªéç©ºè¡Œæ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
    title_line_index = -1
    for i, line in enumerate(lines):
        stripped_line = line.strip()
        if stripped_line and stripped_line.startswith('#'):
            title_line_index = i
            break
    
    # å¦‚æœæ‰¾åˆ°æ ‡é¢˜è¡Œä½†ä¸åŒ…å«æ­£ç¡®çš„è‚¡ç¥¨ä¿¡æ¯ï¼Œåˆ™æ›¿æ¢å®ƒ
    if title_line_index >= 0:
        current_title = lines[title_line_index]
        if ticker not in current_title or stock_name not in current_title:
            lines[title_line_index] = expected_title
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜è¡Œï¼Œåœ¨æŠ¥å‘Šå¼€å¤´æ·»åŠ æ ‡é¢˜
        lines.insert(0, expected_title)
        lines.insert(1, "")  # æ·»åŠ ç©ºè¡Œ
    
    return '\n'.join(lines)

@agent_endpoint("report_analyzer", "è´¢åŠ¡æŠ¥å‘Šåˆ†æåŠ©æ‰‹ï¼Œè´Ÿè´£ç¿»è¯‘è§£è¯»æŠ•èµ„åˆ†ææŠ¥å‘Š")
def report_analyzer_agent(state: AgentState) -> AgentState:
    """
    è§£ææŠ•èµ„åˆ†ææŠ¥å‘Šï¼Œå°†è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¹¶æä¾›è§£è¯»
    
    Args:
        state: å½“å‰AgentçŠ¶æ€å¯¹è±¡
        
    Returns:
        æ›´æ–°åçš„çŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å«ä¸­æ–‡æŠ¥å‘Šå†…å®¹
    """
    # è·å–è‚¡ç¥¨ä»£ç 
    ticker = state.get("data", {}).get("ticker", "æœªçŸ¥è‚¡ç¥¨")
    
    # è¯»å–ç»“æ„åŒ–æ—¥å¿—æ–‡ä»¶ï¼ˆå¸¦æœ‰è‚¡ç¥¨ä»£ç åç¼€ï¼‰
    ticker_suffix = f"_{ticker}" if ticker else ""
    log_file_path = f"logs/structured_terminal{ticker_suffix}.log"
    
    log_content = ""
    try:
        with open(log_file_path, "r", encoding="utf-8") as f:
            log_content = f.read()
        logger.info(f"âœ“ æˆåŠŸè¯»å–æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    except Exception as e:
        # å¦‚æœå¸¦åç¼€çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å–ä¸å¸¦åç¼€çš„é»˜è®¤æ–‡ä»¶
        default_log_path = "logs/structured_terminal.log"
        logger.warning(f"è¯»å–å¸¦æœ‰è‚¡ç¥¨ä»£ç åç¼€çš„æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•è¯»å–é»˜è®¤æ—¥å¿—æ–‡ä»¶")
        try:
            with open(default_log_path, "r", encoding="utf-8") as f:
                log_content = f.read()
            logger.info(f"âœ“ æˆåŠŸè¯»å–é»˜è®¤æ—¥å¿—æ–‡ä»¶: {default_log_path}")
        except Exception as e2:
            logger.error(f"è¯»å–é»˜è®¤æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e2}")
            # è¿”å›é”™è¯¯æ¶ˆæ¯
            from langchain_core.messages import HumanMessage
            messages = state.get("messages", [])
            messages.append(HumanMessage(content=f"æ— æ³•è¯»å–åˆ†ææŠ¥å‘Šæ—¥å¿—ï¼Œè¯·ç¡®ä¿å…ˆè¿è¡Œè‚¡ç¥¨åˆ†ææµç¨‹: {e2}"))
            return {
                "messages": messages,
                "data": state.get("data", {}),
                "metadata": state.get("metadata", {})
            }
    
    # æå–æŠ¥å‘Šå„ç« èŠ‚
    try:
        sections = extract_sections(log_content)
        logger.info(f"âœ“ å·²ä»æ—¥å¿—ä¸­æå– {len(sections)} ä¸ªç« èŠ‚")
        
        # æå–æœ€ç»ˆå†³ç­–
        final_action, confidence = extract_final_decision(log_content)
        logger.info(f"âœ“ æœ€ç»ˆå†³ç­–: {final_action}, ç½®ä¿¡åº¦: {confidence}")
    except Exception as e:
        logger.error(f"è§£ææŠ¥å‘Šç« èŠ‚æ—¶å‡ºé”™: {e}")
        sections = []
        final_action, confidence = "æœªçŸ¥", None
    
    # æ ¹æ®è‚¡ç¥¨ä»£ç æŸ¥è¯¢è‚¡ç¥¨åç§°
    stock_name = "å¹³å®‰é“¶è¡Œ"  # é»˜è®¤åç§°ï¼Œé¿å…ä½¿ç”¨æœªçŸ¥å…¬å¸
    try:
        # é€šè¿‡APIæˆ–æœ¬åœ°æ•°æ®è·å–è‚¡ç¥¨åç§°
        import akshare as ak
        try:
            stock_info = ak.stock_individual_info_em(symbol=ticker)
            if not stock_info.empty and stock_info.shape[1] > 1:
                # ç¡®ä¿è·å–åˆ°çš„æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯æµ®ç‚¹æ•°
                name_value = stock_info.iloc[0, 1]
                if isinstance(name_value, (int, float)):
                    # å¦‚æœè·å–åˆ°çš„æ˜¯æ•°å€¼ï¼Œå¯èƒ½æ˜¯ä»·æ ¼ï¼Œä½¿ç”¨é»˜è®¤åç§°
                    logger.warning(f"è·å–åˆ°çš„è‚¡ç¥¨åç§°ä¼¼ä¹æ˜¯æ•°å€¼: {name_value}ï¼Œå°†ä½¿ç”¨é»˜è®¤åç§°")
                else:
                    stock_name = str(name_value)
            logger.info(f"âœ“ è·å–åˆ°è‚¡ç¥¨åç§°: {stock_name}")
        except Exception as e_info:
            logger.warning(f"é€šè¿‡APIè·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e_info}")
            
            # å°è¯•ä»å¸¸è§è‚¡ç¥¨ä»£ç æ˜ å°„ä¸­è·å–
            stock_code_map = {
                "000001": "å¹³å®‰é“¶è¡Œ",
                "600000": "æµ¦å‘é“¶è¡Œ",
                "601398": "å·¥å•†é“¶è¡Œ",
                "601988": "ä¸­å›½é“¶è¡Œ",
                # å¯ä»¥æ·»åŠ æ›´å¤šæ˜ å°„
            }
            if ticker in stock_code_map:
                stock_name = stock_code_map[ticker]
                logger.info(f"âœ“ ä»é¢„å®šä¹‰æ˜ å°„è·å–è‚¡ç¥¨åç§°: {stock_name}")
    except Exception as e:
        logger.warning(f"è·å–è‚¡ç¥¨åç§°å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤åç§°: {stock_name}")
    
    # å‡†å¤‡æç¤ºè¯
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œè´Ÿè´£è§£è¯»Aè‚¡æŠ•èµ„åˆ†ææŠ¥å‘Šã€‚
    è¯·å°†ä»¥ä¸‹æŠ•èµ„åˆ†ææŠ¥å‘Šç¿»è¯‘æˆç®€æ´æ˜äº†çš„ä¸­æ–‡ï¼Œè§£é‡Šå„é¡¹æŒ‡æ ‡çš„å«ä¹‰ã€è®¡ç®—æ–¹æ³•å’ŒæŠ•èµ„å»ºè®®ã€‚
    
    ç‰¹åˆ«è¦æ±‚:
    1. ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¢å‘æ™®é€šæŠ•èµ„è€…
    2. ä¿ç•™æ‰€æœ‰å…³é”®æ•°æ®å’ŒæŒ‡æ ‡ï¼Œä½†ç”¨ä¸­æ–‡è§£é‡Šå…¶å«ä¹‰
    3. æŒ‰ç…§åŸå§‹æŠ¥å‘Šçš„ç»“æ„ç»„ç»‡å†…å®¹
    4. æŒ‡å‡ºå„ä¸ªéƒ¨åˆ†çš„åˆ†æç»“è®ºä¹‹é—´çš„å…³è”æ€§
    5. è§£é‡Šæ¯ä¸ªæŒ‡æ ‡æ˜¯å¦‚ä½•è®¡ç®—å¾—å‡ºçš„ï¼Œç”±å“ªä¸ªæ¨¡å—æˆ–ä»£ç ç”Ÿæˆ
    6. ä»¥markdownæ ¼å¼è¾“å‡ºç»“æœ
    7. å¯¹äºæŠ€æœ¯åˆ†æéƒ¨åˆ†ï¼Œéœ€è¦è§£é‡ŠADXã€RSIã€å¸ƒæ—å¸¦ç­‰æŠ€æœ¯æŒ‡æ ‡çš„å«ä¹‰
    8. å¯¹äºä¼°å€¼åˆ†æéƒ¨åˆ†ï¼Œéœ€è¦è§£é‡ŠDCFå’Œæ‰€æœ‰è€…æ”¶ç›Šåˆ†ææ–¹æ³•çš„åŒºåˆ«
    9. æ ‡é¢˜å¿…é¡»æ˜¯"# {ticker} {stock_name}æŠ•èµ„åˆ†ææŠ¥å‘Š"ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–æ ‡é¢˜
    10. åœ¨æŠ¥å‘Šç»“å°¾æ€»ç»“å…³é”®æŠ•èµ„è¦ç‚¹å’Œé£é™©æç¤º
    
    åˆ†æç›®æ ‡è‚¡ç¥¨: {ticker} {stock_name}
    æœ€ç»ˆæŠ•èµ„å†³ç­–: {final_action} {'ï¼Œç½®ä¿¡åº¦ ' + str(int(confidence * 100)) + '%' if confidence is not None else ''}
    
    æŠ¥å‘Šå†…å®¹:
    {log_content}
    """
    
    # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘å’Œè§£è¯»
    try:
        report_analysis = get_chat_completion(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œæ“…é•¿ç¿»è¯‘å’Œè§£è¯»æŠ•èµ„åˆ†ææŠ¥å‘Šã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è‹±æ–‡åˆ†ææŠ¥å‘Šç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶è§£é‡ŠæŒ‡æ ‡æ¥æºå’Œè®¡ç®—æ–¹æ³•ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.3,
            client_type="siliconflow"  # æ˜¾å¼æŒ‡å®šä½¿ç”¨ç¡…åŸºæµåŠ¨API
        )
    except Exception as e:
        logger.error(f"è°ƒç”¨LLMè¿›è¡ŒæŠ¥å‘Šåˆ†æå¤±è´¥: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages = state.get("messages", [])
        messages.append(HumanMessage(content=f"è°ƒç”¨LLMè¿›è¡ŒæŠ¥å‘Šåˆ†æå¤±è´¥: {e}"))
        return {
            "messages": messages,
            "data": state.get("data", {}),
            "metadata": state.get("metadata", {})
        }
    
    # ç¡®ä¿æŠ¥å‘Šæ ‡é¢˜æ­£ç¡®
    report_analysis = ensure_correct_report_title(report_analysis, ticker, stock_name)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_dir = "result"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ–‡ä»¶ååŒ…å«è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸï¼Œä½†ä¸å«åç§°ä»¥é¿å…è·¯å¾„è¿‡é•¿
    date_str = datetime.now().strftime('%Y%m%d')
    price_suffix = ""
    try:
        # å°è¯•è·å–å½“å‰ä»·æ ¼ä½œä¸ºæ–‡ä»¶ååç¼€
        price_text = ""
        price_pattern = r"å½“å‰ä»·æ ¼.*?(\d+\.\d+)"
        price_match = re.search(price_pattern, log_content, re.IGNORECASE)
        if price_match:
            price_text = price_match.group(1)
            price_suffix = f"_{price_text}"
    except:
        pass
    
    output_file = os.path.join(output_dir, f"è‚¡ç¥¨{ticker}{price_suffix}_åˆ†ææŠ¥å‘Š_{date_str}.md")
    
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(report_analysis)
        logger.info(f"âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ {output_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    # æ›´æ–°æ•°æ®
    data = state.get("data", {})
    data["report_analysis"] = report_analysis
    data["report_file_path"] = output_file
    data["final_action"] = final_action
    if confidence is not None:
        data["action_confidence"] = confidence
    
    # æ·»åŠ æ€»ç»“æ¶ˆæ¯
    summary_message = f"å·²å®Œæˆè‚¡ç¥¨ {ticker} ({stock_name}) æŠ•èµ„æŠ¥å‘Šçš„ä¸­æ–‡ç¿»è¯‘å’Œè§£è¯»ï¼Œæœ€ç»ˆæŠ•èµ„å»ºè®®ä¸º {final_action}ï¼Œç»“æœä¿å­˜åœ¨: {output_file}"
    from langchain_core.messages import HumanMessage
    messages = state.get("messages", [])
    messages.append(HumanMessage(content=summary_message))
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "messages": messages,
        "data": data,
        "metadata": state.get("metadata", {})
    } 